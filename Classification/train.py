import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
import os
import time
import copy

# ==========================================
# 1. ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ùˆ ØªØ´Ø®ÛŒØµ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±
# ==========================================
# Ù…Ø³ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡ (Train/Val)
DATA_DIR = './dataset/5- slplited/' 

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GPU Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Device being used: {device}")

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯ÛŒØªØ§Ø³Øª
if not os.path.exists(DATA_DIR):
    raise FileNotFoundError(f"Ù¾ÙˆØ´Ù‡ {DATA_DIR} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯! Ù„Ø·ÙØ§ Ø§Ø¨ØªØ¯Ø§ Ú©Ø¯ split-folders Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")

# ==========================================
# 2. ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¢Ù…ÙˆØ²Ø´ (Ú©Ø§Ù…Ù„Ø§ Ø®ÙˆØ¯Ú©Ø§Ø±)
# ==========================================
def train_model_with_params(batch_size, num_epochs):
    print(f"\n{'='*40}")
    print(f"ğŸš€ STARTING TRAINING | Batch: {batch_size} | Epochs: {num_epochs}")
    print(f"{'='*40}")

    # Ø§Ù„Ù) Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Data Augmentation)
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(), # Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ØµÙ†ÙˆØ¹ÛŒ
            transforms.RandomRotation(15),     # Ú©Ù…ÛŒ Ú†Ø±Ø®Ø´
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x]) for x in ['train', 'val']}
    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']}
    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
    class_names = image_datasets['train'].classes
    num_classes = len(class_names)

    # Ø¨) Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø®Ø§Ù… (ResNet18)
    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, num_classes) # ØªØºÛŒÛŒØ± Ù„Ø§ÛŒÙ‡ Ø¢Ø®Ø±
    model = model.to(device)

    # Ø¬) ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² Ùˆ ØªØ§Ø¨Ø¹ Ø®Ø·Ø§
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    
    # Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø§ÛŒÙ† Ø§Ø¬Ø±Ø§
    final_train_loss = 0.0
    final_train_acc = 0.0
    final_val_loss = 0.0
    final_val_acc = 0.0

    start_time = time.time()

    # Ø¯) Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´ (Epochs)
    for epoch in range(num_epochs):
        # Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª Ù‡Ø± 5 Ø¯ÙˆØ± ÛŒÚ©Ø¨Ø§Ø± (Ø¨Ø±Ø§ÛŒ Ø´Ù„ÙˆØº Ù†Ø´Ø¯Ù† Ú©Ù†Ø³ÙˆÙ„)
        if (epoch+1) % 5 == 0 or epoch == 0:
            print(f'Epoch {epoch+1}/{num_epochs}')

        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            # Ù¾ÛŒÙ…Ø§ÛŒØ´ Ø±ÙˆÛŒ Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ (Batches)
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            # Ø°Ø®ÛŒØ±Ù‡ Ø¢Ù…Ø§Ø± Ø¢Ø®Ø±ÛŒÙ† Ø¯ÙˆØ± Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´
            if phase == 'train':
                final_train_loss = epoch_loss
                final_train_acc = epoch_acc
            else:
                final_val_loss = epoch_loss
                final_val_acc = epoch_acc
                
                # *** Ø¨Ø®Ø´ Ø·Ù„Ø§ÛŒÛŒ: Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ ***
                if epoch_acc > best_acc:
                    best_acc = epoch_acc
                    best_model_wts = copy.deepcopy(model.state_dict())
                    # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Øª Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ØŒ Ø¯Ø± Ø¢Ø®Ø± ÙØ§ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                    print(f"   --> New Best Acc: {best_acc:.4f} found at Epoch {epoch+1}")

    # Ù‡Ù€) Ù¾Ø§ÛŒØ§Ù† Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„
    time_elapsed = time.time() - start_time
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:4f}')

    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø±ÙˆÛŒ Ù…Ø¯Ù„
    model.load_state_dict(best_model_wts)
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ ÙÛŒØ²ÛŒÚ©ÛŒ
    save_path = f"./models/model_bs{batch_size}_ep{num_epochs}_trAcc{final_train_acc:.4f}_valAcc{final_val_acc:.4f}.pth"
    torch.save(model.state_dict(), save_path)
    print(f"âœ… Saved Best Model to: {save_path}")

    return {
        'bs': batch_size,
        'ep': num_epochs,
        'train_loss': final_train_loss,
        'train_acc': final_train_acc,
        'val_loss': final_val_loss,
        'val_acc': final_val_acc,
        'best_val_acc': best_acc # Ø¯Ù‚Øª Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ÛŒ Ú©Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯
    }

# ==========================================
# 3. Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù (Main Block)
# ==========================================
if __name__ == "__main__":
    batch_size_list = [8, 16, 32, 64] 
    epoch_list = [16, 18, 20 ,22, 24, 26, 28, 30, 32, 34, 36, 40, 42 , 44]

    results_list = []

    # Ø§Ø¬Ø±Ø§ÛŒ Ø­Ù„Ù‚Ù‡ Ø±ÙˆÛŒ ØªÙ…Ø§Ù… ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    for bs in batch_size_list:
        for ep in epoch_list:
            try:
                res = train_model_with_params(bs, ep)
                results_list.append(res)
            except Exception as e:
                print(f"âŒ Error with BS={bs}, EP={ep}: {e}")

    # ==========================================
    # 4. Ù†Ù…Ø§ÛŒØ´ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ (Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡)
    # ==========================================
    print("\n\n")
    print("="*80)
    print(f"{'FINAL REPORT':^80}")
    print("="*80)
    print(f"{'Batch':<8} | {'Epoch':<8} | {'Best Val Acc (Saved)':<20} | {'Final Val Loss':<15}")
    print("-" * 80)

    best_config = None
    highest_acc = 0.0

    for r in results_list:
        # ØªØ¨Ø¯ÛŒÙ„ ØªÙ†Ø³ÙˆØ± Ø¨Ù‡ Ø¹Ø¯Ø¯ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ØªÙ…ÛŒØ²
        best_acc_val = r['best_val_acc'].item() if torch.is_tensor(r['best_val_acc']) else r['best_val_acc']
        final_loss_val = r['val_loss'] # Loss Ù…Ø¹Ù…ÙˆÙ„Ø§ float Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø§Ø³Øª

        print(f"{r['bs']:<8} | {r['ep']:<8} | {best_acc_val:.4f}{' (Highest!)' if best_acc_val > highest_acc else '' :<15} | {final_loss_val:.4f}")
        
        if best_acc_val > highest_acc:
            highest_acc = best_acc_val
            best_config = r

    print("="*80)
    if best_config:
        print(f"ğŸ† WINNER CONFIGURATION: Batch Size {best_config['bs']} with {best_config['ep']} Epochs")
        print(f"   Please use file: 'best_model_bs{best_config['bs']}_ep{best_config['ep']}.pth' for your app.")
    print("="*80)
